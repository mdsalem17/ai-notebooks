{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_NMT_EN-to-DE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bnIFtGi332wi",
        "Fwpu5NOMRbIO",
        "JoBrcZ1pD0mm",
        "qeeduDy-R2sA",
        "Fl_9aZZAvwF5",
        "pyi9WhY0vwF8",
        "jpAtzXUgvwGO",
        "Kvt5tVhmvwGQ",
        "seRV_tAWvwGS",
        "-UTYw7Q1vwGT"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYg4dAkaWHAlUp++3ZjbS7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdsalem17/scientific-guide-notebooks/blob/main/machine-translation/NLP_NMT_EN_to_DE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j5sg4BgwpvX"
      },
      "source": [
        "# Neural Machine Translation (NMT)\n",
        "## A List of Abbreviations\n",
        "- MT: Machine Translation\n",
        "- NMT: Neural Machine Translation\n",
        "- LSTM: Long Short-Term Memory\n",
        "- RNN: Recurrent Neural Network\n",
        "- BLEU: Bilingual Evaluation Understudy\n",
        "\n",
        "## Introduction\n",
        "In this notebook, we will interested in building an English-to-German NMT model. In order to do so, we will use an Attention-based LSTM model. Implementing Machine Translation using LSTM models or any RNN models in general will not work effectively for long sentences due to vanishing gradient problem. To overcome this, we can add an attention mechanism to allow the model to access the relevant parts of the input sentence. We will see this in the discussions below. Let's get started!\n",
        "\n",
        "\n",
        "## Warning: \n",
        "In this notebook, we are using [Trax — Deep Learning with Clear Code and Speed](https://github.com/google/trax) library, maintained by Google Brain team.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9xtQc9t-pzQ"
      },
      "source": [
        "# command to install trax if needed\n",
        "#!pip install -U trax"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnIFtGi332wi"
      },
      "source": [
        "## Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f7wL-MOAysr"
      },
      "source": [
        "### Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO4gjoXX3ZFM"
      },
      "source": [
        "from termcolor import colored\n",
        "import random\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kCYQQ7j4LVs"
      },
      "source": [
        "To train our model, we will use a dataset from [Opus](http://opus.nlpl.eu/) available via [Tensorflow Datasets (TFDS)](https://www.tensorflow.org/datasets). The dataset that interests us contains medical related texts in English translated to German."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fmly96Jt9-dz"
      },
      "source": [
        "We can easily download this dataset via TFDS with `trax.data.TFDS`. We can then find it in the directory called `data/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmqhMOQC7aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db81aa3a-3fc5-4dd0-afb9-7bac13860aa5"
      },
      "source": [
        "# trax.data.TFDS return Python generator function yielding tuples\n",
        "#as (English sentence, German sentence).\n",
        "#\n",
        "# This may take a while\n",
        "train_stream_fn = trax.data.TFDS('opus/medical',\n",
        "                                 data_dir='./data/',\n",
        "                                 keys=('en', 'de'),\n",
        "                                 eval_holdout_size=0.01, # 1% for eval\n",
        "                                 train=True)\n",
        "\n",
        "# Get generator function for the eval set\n",
        "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
        "                                data_dir='./data/',\n",
        "                                keys=('en', 'de'),\n",
        "                                eval_holdout_size=0.01, # 1% for eval\n",
        "                                train=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:385: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db4IZB1GAdaw",
        "outputId": "214f7f85-0c35-4af6-a2c9-db13816e42f0"
      },
      "source": [
        "train_stream = train_stream_fn()\n",
        "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
        "print()\n",
        "\n",
        "eval_stream = eval_stream_fn()\n",
        "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'Tel: +421 2 57 103 777\\n', b'Tel: +421 2 57 103 777\\n')\n",
            "\n",
            "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Subcutaneous use and intravenous use.\\n', b'Subkutane Anwendung und intraven\\xc3\\xb6se Anwendung.\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z50SrDDfBBRR"
      },
      "source": [
        "### Formatting the Data\n",
        "We have successfully imported our corpus, now, we can preprocess the sentences into a format suitable to our model. We will do that in 3 steps:\n",
        "\n",
        "1.   **Tokenizing the sentences using subword representations**: \n",
        "We will represent each sentence as an array of integers instead of strings. We will use subword representation to tokenize our sentences that will allow us to avoid out-of-vocabulary words. The Tokenizing can be done with the `trax.data.Tokenize()`.\n",
        "2.   **Appending an end-of-sentence token to each sentence**:\n",
        "We will assign a specific token (`1`) to mark the end of a sentence in order to know the model has completed the translation.\n",
        "\n",
        "3.   **Filtering long sentences**: We will use `trax.data.FilterByLength()` to place a limit on the number of tokens per sentence to ensure we won't run out of memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYR2e8piJ6Zy"
      },
      "source": [
        "#### Tokenizing the sentences using subword representations\n",
        "We want create a vocabulary of about 32,000 tokens. In order to get the subword representations need for tokenization, we will need to extract this information from the dataset by mixing together the English sentences with the German sentences to make a vocabulary from both languages.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "3UeP571cFiOz",
        "outputId": "5a414f62-932b-4e4a-d459-7affce88653b"
      },
      "source": [
        "\"\"\"#subword representation\n",
        "i = 0 \n",
        "with open('data/train.txt', 'w') as f: \n",
        "    for text_en, text_es in train_stream: \n",
        "        f.write(str(text_en, 'utf-8') + '\\n') \n",
        "        f.write(str(text_es, 'utf-8') + '\\n') \n",
        "        if i == 1000000: \n",
        "            break\n",
        "        i += 1\"\"\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"#subword representation\\ni = 0 \\nwith open('data/train.txt', 'w') as f: \\n    for text_en, text_es in train_stream: \\n        f.write(str(text_en, 'utf-8') + '\\n') \\n        f.write(str(text_es, 'utf-8') + '\\n') \\n        if i == 1000000: \\n            break\\n        i += 1\""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLkU1cttHRZa"
      },
      "source": [
        "#!wc -l data/train.txt"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPsl-HHoPV79"
      },
      "source": [
        "We will then create the vocabulary using a script from Tensor2Tensor  to Trax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-RxOj-ZIfYs"
      },
      "source": [
        "#!git clone https://github.com/google/trax.git"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "3L5IzKnoIm_N",
        "outputId": "752305f2-9b36-4e83-8c04-98b4e2f51049"
      },
      "source": [
        "\"\"\"!python trax/trax/data/text_encoder_build_subword.py \\\n",
        "--corpus_filepattern=data/train.txt --corpus_max_lines=40000 \\\n",
        "--output_filename=data/ende_32k.subword\"\"\""
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!python trax/trax/data/text_encoder_build_subword.py --corpus_filepattern=data/train.txt --corpus_max_lines=40000 --output_filename=data/ende_32k.subword'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVAJK8b9e9Za",
        "outputId": "8b95fedc-4fc8-47c6-d138-19b614b89b11"
      },
      "source": [
        "!wget http://salem.scientific-guide.com/wp-content/uploads/2021/10/ende_32k.subword.zip\n",
        "!unzip ende_32k.subword.zip"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-29 17:49:07--  http://salem.scientific-guide.com/wp-content/uploads/2021/10/ende_32k.subword.zip\n",
            "Resolving salem.scientific-guide.com (salem.scientific-guide.com)... 217.160.0.195, 2001:8d8:100f:f000::234\n",
            "Connecting to salem.scientific-guide.com (salem.scientific-guide.com)|217.160.0.195|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 127090 (124K) [application/zip]\n",
            "Saving to: ‘ende_32k.subword.zip’\n",
            "\n",
            "\rende_32k.subword.zi   0%[                    ]       0  --.-KB/s               \rende_32k.subword.zi 100%[===================>] 124.11K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-10-29 17:49:08 (2.09 MB/s) - ‘ende_32k.subword.zip’ saved [127090/127090]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scdK1p0MK4L9",
        "outputId": "b53fc310-4816-41f1-dea5-127c5774aed2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data\t\t      __MACOSX\t   sample_data\t   w1_unittest.py.1\n",
            "ende_32k.subword      output_dir   trax\t\t   w1_unittest.py.2\n",
            "ende_32k.subword.zip  __pycache__  w1_unittest.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxJPCLwJfADp"
      },
      "source": [
        "!mv ende_32k.subword data/ende_32k.subword"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPE5vBzzD1Hm"
      },
      "source": [
        "# global variables\n",
        "VOCAB_FILE = 'ende_32k.subword'\n",
        "VOCAB_DIR = 'data/'\n",
        "\n",
        "# Tokenize the dataset.\n",
        "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
        "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwpu5NOMRbIO"
      },
      "source": [
        "#### Appending an end-of-sentence token to each sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fV5WEQTED8O"
      },
      "source": [
        "# end-of-sentence (EOS) Integer\n",
        "EOS = 1\n",
        "\n",
        "# generator helper function to append EOS to each sentence\n",
        "def append_eos(stream):\n",
        "    for (inputs, targets) in stream:\n",
        "        inputs_with_eos = list(inputs) + [EOS]\n",
        "        targets_with_eos = list(targets) + [EOS]\n",
        "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
        "\n",
        "# append EOS to the train data\n",
        "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
        "\n",
        "# append EOS to the eval data\n",
        "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoBrcZ1pD0mm"
      },
      "source": [
        "#### Filtering long sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOMiDRxJEO6m",
        "outputId": "87364dee-765d-4a95-9bc6-38a1dc076c46"
      },
      "source": [
        "# Filter too long sentences to not run out of memory.\n",
        "# length_keys=[0, 1] means we filter both English and German sentences, so\n",
        "# both much be not longer that 256 tokens for training / 512 for eval.\n",
        "filtered_train_stream = trax.data.FilterByLength(\n",
        "    max_length=256, length_keys=[0, 1])(tokenized_train_stream)\n",
        "filtered_eval_stream = trax.data.FilterByLength(\n",
        "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n",
        "\n",
        "# print a sample input-target pair of tokenized sentences\n",
        "train_input, train_target = next(filtered_train_stream)\n",
        "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
        "print(colored(f'Single tokenized example target:', 'red'), train_target)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mSingle tokenized example input:\u001b[0m [ 2538  2248    30 12114 23184 16889     5     2 20852  6456 20592  5812\n",
            "  3932    96  5178  3851    30  7891  3550 30650  4729   992     1]\n",
            "\u001b[31mSingle tokenized example target:\u001b[0m [ 1872    11  3544    39  7019 17877 30432    23  6845    10 14222    47\n",
            "  4004    18 21674     5 27467  9513   920   188 10630    18  3550 30650\n",
            "  4729   992     1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeeduDy-R2sA"
      },
      "source": [
        "### Helper functions\n",
        "We will implement tokenize and detokenize functions to help us map words to their indices, and vice-versa.\n",
        "- tokenize(): converts a text sentence to its corresponding. Also converts words to subwords (parts of words).\n",
        "- detokenize(): converts a token list to its corresponding string or sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecQ0wb3xSsvP"
      },
      "source": [
        "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Encodes a string to an array of integers\n",
        "\n",
        "    Args:\n",
        "        input_str (str): human-readable string to encode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "  \n",
        "    Returns:\n",
        "        numpy.ndarray: tokenized version of the input string\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set the \"end of sentence\" encoding to 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # Use the trax.data.tokenize method.\n",
        "    # It takes streams and returns streams,\n",
        "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
        "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
        "    \n",
        "    # Mark the end of the sentence with EOS\n",
        "    inputs = list(inputs) + [EOS]\n",
        "    \n",
        "    # Adding the batch dimension to the front of the shape\n",
        "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
        "    \n",
        "    return batch_inputs\n",
        "\n",
        "\n",
        "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Decodes an array of integers to a human readable string\n",
        "\n",
        "    Args:\n",
        "        integers (numpy.ndarray): array of integers to decode\n",
        "        vocab_file (str): filename of the vocabulary text file\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "  \n",
        "    Returns:\n",
        "        str: the decoded sentence.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Remove the dimensions of size 1\n",
        "    print(integers)\n",
        "    integers = list(np.squeeze(integers))\n",
        "    \n",
        "    # Set the \"end of sentence\" encoding to 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # Remove the EOS to decode only the original tokens\n",
        "    if EOS in integers:\n",
        "        integers = integers[:integers.index(EOS)] \n",
        "    \n",
        "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QhRFcsKTH8i"
      },
      "source": [
        "Let's see how we might use these functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuSp9HhcTK0Z",
        "outputId": "3970eaa2-8640-4866-9e28-00d54baaf904"
      },
      "source": [
        "# Detokenize an input-target pair of tokenized sentences\n",
        "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print()\n",
        "\n",
        "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
        "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
        "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
        "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2538  2248    30 12114 23184 16889     5     2 20852  6456 20592  5812\n",
            "  3932    96  5178  3851    30  7891  3550 30650  4729   992     1]\n",
            "\u001b[31mSingle detokenized example input:\u001b[0m During treatment with olanzapine, adolescents gained significantly more weight compared with adults.\n",
            "\n",
            "[ 1872    11  3544    39  7019 17877 30432    23  6845    10 14222    47\n",
            "  4004    18 21674     5 27467  9513   920   188 10630    18  3550 30650\n",
            "  4729   992     1]\n",
            "\u001b[31mSingle detokenized example target:\u001b[0m Während der Behandlung mit Olanzapin nahmen die Jugendlichen im Vergleich zu Erwachsenen signifikant mehr Gewicht zu.\n",
            "\n",
            "\n",
            "\u001b[32mtokenize('hello'): \u001b[0m [[17332   140     1]]\n",
            "[17332, 140, 1]\n",
            "\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl_9aZZAvwF5"
      },
      "source": [
        "### Bucketing\n",
        "To speed up the training in NLP, we generally perform a  bucketing of the tokenized sentences. In a nutshell, instead of padding our tokenized sentences with 0s to the maximum length, we can group our tokenized sentences by length and bucket:\n",
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*hcGuja_d5Z_rFcgwe9dPow.png)\n",
        "\n",
        "We batch the sentences with similar length together (e.g. the blue sentences in the image above) and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows to waste less computation when processing padded sequences.\n",
        "In Trax, it is implemented in the [bucket_by_length](https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJV71tLNvwF5"
      },
      "source": [
        "# Bucketing to create streams of batches.\n",
        "\n",
        "# Buckets are defined in terms of boundaries and batch sizes.\n",
        "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
        "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
        "# between 8 and 16, and so on -- and only 2 if length is over 512.\n",
        "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
        "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
        "\n",
        "# Create the generators.\n",
        "train_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_train_stream)\n",
        "\n",
        "eval_batch_stream = trax.data.BucketByLength(\n",
        "    boundaries, batch_sizes,\n",
        "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
        ")(filtered_eval_stream)\n",
        "\n",
        "# Add masking for the padding (0s).\n",
        "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
        "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVT9DK0_vwF6"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "We will now be displaying some of our data. Let's first get the data generator and get one batch of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmw3yDFIvwF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d3d16e-b8be-4fac-fd59-fd39d4dbf662"
      },
      "source": [
        "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
        "\n",
        "# let's see the data type of a batch\n",
        "print(\"input_batch data type: \", type(input_batch))\n",
        "print(\"target_batch data type: \", type(target_batch))\n",
        "\n",
        "# let's see the shape of this particular batch (batch length, sentence length)\n",
        "print(\"input_batch shape: \", input_batch.shape)\n",
        "print(\"target_batch shape: \", target_batch.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_batch data type:  <class 'numpy.ndarray'>\n",
            "target_batch data type:  <class 'numpy.ndarray'>\n",
            "input_batch shape:  (64, 32)\n",
            "target_batch shape:  (64, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h024COvTvwF7"
      },
      "source": [
        "The `input_batch` and `target_batch` are Numpy arrays containing respectively tokenized English sentences and German sentences. These tokens will later be used to produce embedding vectors for each word in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZaftX01vwF8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "282790ec-8dcf-4649-b0a0-fff1bbec1bd9"
      },
      "source": [
        "# pick a random index < the batch size.\n",
        "index = random.randrange(len(input_batch))\n",
        "\n",
        "# then grab an entry from the input and target batch\n",
        "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
        "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
        "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12708  4011 11553  4673  7948  4206     5 30650  4729   992     1     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
            "\u001b[0m STORING TASMAR\n",
            " \n",
            "\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
            " \u001b[0m [12708  4011 11553  4673  7948  4206     5 30650  4729   992     1     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0] \n",
            "\n",
            "[ 6224   493  3337   693  4673  7948  4206     5  9227  2290  1834 14014\n",
            "  2072 13986  1810  3234  1046  7576 30650  4729   992     1     0     0\n",
            "     0     0     0     0     0     0     0     0]\n",
            "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
            "\u001b[0m WIE IST TASMAR AUFZUBEWAHREN?\n",
            " \n",
            "\n",
            "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
            "\u001b[0m [ 6224   493  3337   693  4673  7948  4206     5  9227  2290  1834 14014\n",
            "  2072 13986  1810  3234  1046  7576 30650  4729   992     1     0     0\n",
            "     0     0     0     0     0     0     0     0] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyi9WhY0vwF8"
      },
      "source": [
        "## Neural Machine Translation with Attention\n",
        "\n",
        "Now that we have the data generators and we preprocessed the data, we will be implementing a NMT model from scratch with attention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd2Ff_5vvwF9"
      },
      "source": [
        "### Attention Overview\n",
        "In Neural Machine Translation, we will typically have a model with an encoder-decoder architecture. As said before, we will be building a RNN that takes in a tokenized version of a sentence in its encoder, and passes it on to the decoder for translation. We will add an attention layer to this RNN model, to make it work bettter for long sentences Attention will allow the decoder to access to all relevant parts of the input sentence.\n",
        "\n",
        "There are different ways to implement attention and the one we'll be using is the Scaled Dot Product Attention which has the form:\n",
        "\n",
        "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
        "\n",
        "For our application, the encoder hidden states will be the keys and values, while the decoder hidden states will be the queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6AFnmGKvwF9"
      },
      "source": [
        "### Helper functions\n",
        "We will implement several functions to help us adjust the encoder output and decoder input for attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PACYiVZ05Q5r"
      },
      "source": [
        "#### Input encoder\n",
        "\n",
        "The input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. It is a [Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) network which uses:\n",
        "\n",
        "   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding): Converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\n",
        "  \n",
        "   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yqRr_n7vwF9"
      },
      "source": [
        "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
        "    \"\"\" Input encoder runs on the input sentence and creates\n",
        "    activations that will be the keys and values for attention.\n",
        "    \n",
        "    Args:\n",
        "        input_vocab_size: int: vocab size of the input\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    Returns:\n",
        "        tl.Serial: The input encoder\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a serial network\n",
        "    input_encoder = tl.Serial( \n",
        "        # create an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(vocab_size=input_vocab_size, d_feature=d_model),\n",
        "        # feed the embeddings to the LSTM layers. It is a stack of n_encoder_layers LSTM layers\n",
        "        [tl.LSTM(n_units=d_model) for _ in range(n_encoder_layers)]\n",
        "    )\n",
        "\n",
        "    return input_encoder"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raT-RPoevwF-"
      },
      "source": [
        "#### Pre-attention decoder\n",
        "\n",
        "The pre-attention decoder runs on the targets and creates activations that are used as queries in attention. This is a Serial network which is composed of the following:\n",
        "\n",
        "   - [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): This pads a token to the beginning of your target tokens (e.g. `[8, 34, 12]` shifted right is `[0, 8, 34, 12]`). This will act like a start-of-sentence token that will be the first input to the decoder. During training, this shift also allows the target tokens to be passed as input to do teacher forcing.\n",
        "\n",
        "   - [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding): Like in the previous function, this converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tl.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding.\n",
        "   \n",
        "   - [tl.LSTM](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM): LSTM layer of size `d_model`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iygR42NsvwF_"
      },
      "source": [
        "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
        "    \"\"\" Pre-attention decoder runs on the targets and creates\n",
        "    activations that are used as queries in attention.\n",
        "    \n",
        "    Args:\n",
        "        mode: str: 'train' or 'eval'\n",
        "        target_vocab_size: int: vocab size of the target\n",
        "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    Returns:\n",
        "        tl.Serial: The pre-attention decoder\n",
        "    \"\"\"\n",
        "    \n",
        "    # create a serial network\n",
        "    pre_attention_decoder = tl.Serial(\n",
        "        # shift right to insert start-of-sentence token and implement\n",
        "        # teacher forcing during training\n",
        "        tl.ShiftRight(mode=mode),\n",
        "        # run an embedding layer to convert tokens to vectors\n",
        "        tl.Embedding(vocab_size=target_vocab_size, d_feature=d_model),\n",
        "        # feed to an LSTM layer\n",
        "        tl.LSTM(n_units=d_model)\n",
        "    )\n",
        "    \n",
        "    return pre_attention_decoder"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZIKtkO_vwGA"
      },
      "source": [
        "#### Preparing the attention input\n",
        "\n",
        "This function will prepare the inputs to the attention layer. We want to take in the encoder and pre-attention decoder activations and assign it to the queries, keys, and values. In addition, another output here will be the mask to distinguish real tokens from padding tokens. This mask will be used internally by Trax when computing the softmax so padding tokens will not have an effect on the computated probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-djhQmitvwGB"
      },
      "source": [
        "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
        "    \"\"\"Prepare queries, keys, values and mask for attention.\n",
        "    \n",
        "    Args:\n",
        "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
        "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
        "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
        "    \n",
        "    Returns:\n",
        "        queries, keys, values and mask for attention.\n",
        "    \"\"\"    \n",
        "    # set the keys and values to the encoder activations\n",
        "    keys = encoder_activations\n",
        "    values = encoder_activations\n",
        "    # set the queries to the decoder activations\n",
        "    queries = decoder_activations\n",
        "    # generate the mask to distinguish real tokens from padding\n",
        "    # hint: inputs is 1 for real tokens and 0 where they are padding\n",
        "    mask = inputs != 0\n",
        "        \n",
        "    # add axes to the mask for attention heads and decoder length.\n",
        "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
        "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
        "    # note: for this assignment, attention heads is set to 1.\n",
        "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
        "    \n",
        "    return queries, keys, values, mask"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDnSYRzmvwGC"
      },
      "source": [
        "### Implementation Overview\n",
        "\n",
        "We are now ready to implement our sequence-to-sequence model with attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0OYHzwtvwGD"
      },
      "source": [
        "We will implement in the `NMTAttn` function below our machine translation model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFUyDAGRvwGD"
      },
      "source": [
        "def NMTAttn(input_vocab_size=33300,\n",
        "            target_vocab_size=33300,\n",
        "            d_model=1024,\n",
        "            n_encoder_layers=2,\n",
        "            n_decoder_layers=2,\n",
        "            n_attention_heads=4,\n",
        "            attention_dropout=0.0,\n",
        "            mode='train'):\n",
        "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
        "\n",
        "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
        "    an English sentence (tokenized) and its translation into German (tokenized).\n",
        "\n",
        "    Args:\n",
        "    input_vocab_size: int: vocab size of the input\n",
        "    target_vocab_size: int: vocab size of the target\n",
        "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
        "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
        "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
        "    n_attention_heads: int: number of attention heads\n",
        "    attention_dropout: float, dropout for the attention layer\n",
        "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
        "\n",
        "    Returns:\n",
        "    A LSTM sequence-to-sequence model with attention.\n",
        "    \"\"\"\n",
        "    \n",
        "    # calling helper function to create layers for the input encoder\n",
        "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
        "\n",
        "    # calling helper function to create layers for the pre-attention decoder\n",
        "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
        "\n",
        "    model = tl.Serial(\n",
        "      tl.Select([0,1,0,1]),\n",
        "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
        "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
        "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
        "      tl.Select([0,2]),\n",
        "      [tl.LSTM(n_units=d_model) for _ in range(n_decoder_layers)],\n",
        "      tl.Dense(target_vocab_size),\n",
        "      tl.LogSoftmax()\n",
        "    )\n",
        "        \n",
        "    return model"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "533DSVn3vwGF"
      },
      "source": [
        "## Training NMT Model\n",
        "\n",
        "In this section, we will be training our model. In trax, we will need to instantiate three classes for this: `TrainTask`, `EvalTask`, and `Loop`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIrtjaHIvwGF"
      },
      "source": [
        "### TrainTask\n",
        "\n",
        "The [TrainTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH917n3-vwGG"
      },
      "source": [
        "train_task = training.TrainTask(    \n",
        "    labeled_data= train_batch_stream,\n",
        "\n",
        "    loss_layer= tl.CrossEntropyLoss(),\n",
        "    \n",
        "    optimizer= trax.optimizers.Adam(0.01),\n",
        "    \n",
        "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000, 0.01),\n",
        "    \n",
        "    n_steps_per_checkpoint= 10,\n",
        ")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8x3kKVYvwGG"
      },
      "source": [
        "### EvalTask\n",
        "\n",
        "The [EvalTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) allows us to see how the model is doing while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaEZbESivwGH"
      },
      "source": [
        "eval_task = training.EvalTask(\n",
        "    \n",
        "    ## use the eval batch stream as labeled data\n",
        "    labeled_data=eval_batch_stream,\n",
        "    \n",
        "    ## use the cross entropy loss and accuracy as metrics\n",
        "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyeHs2GCvwGH"
      },
      "source": [
        "### Loop\n",
        "\n",
        "The [Loop](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) class defines the model we will train as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "QCS_9a2uvwGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ec1519b-6ae6-4ac1-9dbe-45e4a7921e06"
      },
      "source": [
        "# define the output directory\n",
        "output_dir = 'output_dir/'\n",
        "\n",
        "# remove old model if it exists. restarts training.\n",
        "!rm -f ~/output_dir/model.pkl.gz  \n",
        "\n",
        "# define the training loop\n",
        "training_loop = training.Loop(NMTAttn(mode='train'),\n",
        "                              train_task,\n",
        "                              eval_tasks=[eval_task],\n",
        "                              output_dir=output_dir)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:385: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqCgwDVzvwGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "610902c3-9d4f-4b4a-a4c8-4172af298826"
      },
      "source": [
        "# Execute the training loop. This will take about 9-10 minutes to complete.\n",
        "training_loop.run(10)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step     30: Ran 10 train steps in 531.56 secs\n",
            "Step     30: train CrossEntropyLoss |  8.53586102\n",
            "Step     30: eval  CrossEntropyLoss |  7.50082159\n",
            "Step     30: eval          Accuracy |  0.03472714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A3vaHhzvwGK"
      },
      "source": [
        "## Testing\n",
        "\n",
        "We will now be using the model we just trained to translate English sentences to German. We will implement this with two functions: The first allows us to identify the next symbol (i.e. output token). The second one takes care of combining the entire translated string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l12lALVPvwGK"
      },
      "source": [
        "# instantiate the model we built in eval mode\n",
        "model = NMTAttn(mode='eval')\n",
        "\n",
        "# initialize weights from a pre-trained model\n",
        "model.init_from_file(\"output_dir/model.pkl.gz\", weights_only=True)\n",
        "model = tl.Accelerate(model)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoQb8u9FvwGL"
      },
      "source": [
        "### Decoding\n",
        "\n",
        "There are several ways to get the next token when translating a sentence. For instance, we can just get the most probable token at each step (i.e. greedy decoding) or get a sample from a distribution. We can generalize the implementation of these two approaches by using the `tl.logsoftmax_sample()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8NABj0PvwGL"
      },
      "source": [
        "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
        "    \"\"\"Returns the index of the next token.\n",
        "\n",
        "    Args:\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
        "        cur_output_tokens (list): tokenized representation of previously translated words\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        int: index of the next token in the translated sentence\n",
        "        float: log probability of the next symbol\n",
        "    \"\"\"\n",
        "    \n",
        "    # set the length of the current output tokens\n",
        "    token_length = len(cur_output_tokens)\n",
        "\n",
        "    # calculate next power of 2 for padding length \n",
        "    padded_length = np.power(2, int(np.ceil(np.log2(token_length + 1))))\n",
        "\n",
        "    # pad cur_output_tokens up to the padded_length\n",
        "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
        "    \n",
        "    # model expects the output to have an axis for the batch size in front so\n",
        "    # convert `padded` list to a numpy array with shape (x, <padded_length>) where the\n",
        "    # x position is the batch axis. (hint: you can use np.expand_dims() with axis=0 to insert a new axis)\n",
        "    padded_with_batch = np.expand_dims(padded, axis=0)\n",
        "\n",
        "    # get the model prediction. remember to use the `NMTAttn` argument defined above.\n",
        "    # hint: the model accepts a tuple as input (e.g. `my_model((input1, input2))`)\n",
        "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
        "    \n",
        "    # get log probabilities from the last token output\n",
        "    log_probs = output[0, token_length, :]\n",
        "\n",
        "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
        "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
        "    \n",
        "    return symbol, float(log_probs[symbol])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omnvVWxtvwGL"
      },
      "source": [
        "Now you will implement the `sampling_decode()` function. This will call the `next_symbol()` function above several times until the next output is the end-of-sentence token (i.e. `EOS`). It takes in an input string and returns the translated version of that string.\n",
        "\n",
        "<a name=\"ex07\"></a>\n",
        "### Exercise 07\n",
        "\n",
        "**Instructions**: Implement the `sampling_decode()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymFHrI7vwGM"
      },
      "source": [
        "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Returns the translated sentence.\n",
        "\n",
        "    Args:\n",
        "        input_sentence (str): sentence to translate.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (list, str, float)\n",
        "            list of int: tokenized version of the translated sentence\n",
        "            float: log probability of the translated sentence\n",
        "            str: the translated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    # encode the input sentence\n",
        "    input_tokens = tokenize(input_sentence,vocab_file,vocab_dir)\n",
        "    \n",
        "    # initialize the list of output tokens\n",
        "    cur_output_tokens = []\n",
        "    \n",
        "    # initialize an integer that represents the current output index\n",
        "    cur_output = 0\n",
        "    \n",
        "    # Set the encoding of the \"end of sentence\" as 1\n",
        "    EOS = 1\n",
        "    \n",
        "    # check that the current output is not the end of sentence token\n",
        "    while cur_output != EOS:\n",
        "        \n",
        "        # update the current output token by getting the index of the next word (hint: use next_symbol)\n",
        "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
        "        \n",
        "        # append the current output token to the list of output tokens\n",
        "        cur_output_tokens.append(cur_output)\n",
        "    \n",
        "    # detokenize the output tokens\n",
        "    sentence = detokenize(cur_output_tokens, vocab_file, vocab_dir)\n",
        "    \n",
        "    return cur_output_tokens, log_prob, sentence\n",
        "\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVELottRvwGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34dd94c7-68d7-4214-b982-2c7df0be8f9a"
      },
      "source": [
        "# Test the function above. Try varying the temperature setting with values from 0 to 1.\n",
        "# Run it several times with each setting and see how often the output changes.\n",
        "sampling_decode(\"I love languages.\", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  5,\n",
              "  1],\n",
              " -4.988891124725342,\n",
              " '')"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "963umPCBvwGN"
      },
      "source": [
        "We have set a default value of `0` to the temperature setting in our implementation of `sampling_decode()` above. As you may have noticed in the `logsoftmax_sample()` method, this setting will ultimately result in greedy decoding. As mentioned in the lectures, this algorithm generates the translation by getting the most probable word at each step. It gets the argmax of the output array of your model and then returns that index. See the testing function and sample inputs below. You'll notice that the output will remain the same each time you run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXRymRYrvwGN"
      },
      "source": [
        "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Prints the input and output of our NMTAttn model using greedy decode\n",
        "\n",
        "    Args:\n",
        "        sentence (str): a custom string.\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"        \n",
        "    _,_, translated_sentence = sampling_decode(sentence, NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "    \n",
        "    print(\"English: \", sentence)\n",
        "    print(\"German: \", translated_sentence)\n",
        "    \n",
        "    return translated_sentence"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzxP6lmDvwGO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "90f1e997-eb64-4e39-fe40-f8f6949db999"
      },
      "source": [
        "# put a custom string here\n",
        "your_sentence = 'I love languages.'\n",
        "\n",
        "greedy_decode_test(your_sentence, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:  I love languages.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-6766d0a504fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myour_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'I love languages.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgreedy_decode_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myour_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-9bf14bc3fe35>\u001b[0m in \u001b[0;36mgreedy_decode_test\u001b[0;34m(sentence, NMTAttn, vocab_file, vocab_dir)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"English: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNMTAttn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"English: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-26bad0125997>\u001b[0m in \u001b[0;36msampling_decode\u001b[0;34m(input_sentence, NMTAttn, temperature, vocab_file, vocab_dir)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# update the current output token by getting the index of the next word (hint: use next_symbol)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mcur_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_symbol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNMTAttn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# append the current output token to the list of output tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-2bae9d7b508e>\u001b[0m in \u001b[0;36mnext_symbol\u001b[0;34m(NMTAttn, input_tokens, cur_output_tokens, temperature)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# get the model prediction. remember to use the `NMTAttn` argument defined above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# hint: the model accepts a tuple as input (e.g. `my_model((input1, input2))`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMTAttn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_with_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# get log probabilities from the last token output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, weights, state, rng)\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m  \u001b[0;31m# Needed if the model wasn't fully initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/trax/layers/acceleration.py\u001b[0m in \u001b[0;36mpure_fn\u001b[0;34m(self, x, weights, state, rng, use_cache)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0mremainder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremainder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# If yes, run the accelerated sublayer.pure_fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pure_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;31m# If not, pad first.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOt1xyC-vwGO"
      },
      "source": [
        "greedy_decode_test('You are almost done with the assignment!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyBX9MDdvwGO"
      },
      "source": [
        "<a name=\"4.2\"></a>\n",
        "## 4.2  Minimum Bayes-Risk Decoding\n",
        "\n",
        "As mentioned in the lectures, getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:\n",
        "\n",
        "1. take several random samples\n",
        "2. score each sample against all other samples\n",
        "3. select the one with the highest score\n",
        "\n",
        "You will be building helper functions for these steps in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpAtzXUgvwGO"
      },
      "source": [
        "<a name='4.2.1'></a>\n",
        "### 4.2.1 Generating samples\n",
        "\n",
        "First, let's build a function to generate several samples. You can use the `sampling_decode()` function you developed earlier to do this easily. We want to record the token list and log probability for each sample as these will be needed in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klH3pQ5UvwGP"
      },
      "source": [
        "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Generates samples using sampling_decode()\n",
        "\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "        \n",
        "    Returns:\n",
        "        tuple: (list, list)\n",
        "            list of lists: token list per sample\n",
        "            list of floats: log probability per sample\n",
        "    \"\"\"\n",
        "    # define lists to contain samples and probabilities\n",
        "    samples, log_probs = [], []\n",
        "\n",
        "    # run a for loop to generate n samples\n",
        "    for _ in range(n_samples):\n",
        "        \n",
        "        # get a sample using the sampling_decode() function\n",
        "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "        \n",
        "        # append the token list to the samples list\n",
        "        samples.append(sample)\n",
        "        \n",
        "        # append the log probability to the log_probs list\n",
        "        log_probs.append(logp)\n",
        "                \n",
        "    return samples, log_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-3LIUrMvwGP"
      },
      "source": [
        "# generate 4 samples with the default temperature (0.6)\n",
        "generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvt5tVhmvwGQ"
      },
      "source": [
        "### 4.2.2 Comparing overlaps\n",
        "\n",
        "Let us now build our functions to compare a sample against another. There are several metrics available as shown in the lectures and you can try experimenting with any one of these. For this assignment, we will be calculating scores for unigram overlaps. One of the more simple metrics is the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) which gets the intersection over union of two sets. We've already implemented it below for your perusal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA59zxFEvwGQ"
      },
      "source": [
        "def jaccard_similarity(candidate, reference):\n",
        "    \"\"\"Returns the Jaccard similarity between two token lists\n",
        "\n",
        "    Args:\n",
        "        candidate (list of int): tokenized version of the candidate translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"\n",
        "    \n",
        "    # convert the lists to a set to get the unique tokens\n",
        "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n",
        "    \n",
        "    # get the set of tokens common to both candidate and reference\n",
        "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
        "    \n",
        "    # get the set of all tokens found in either candidate or reference\n",
        "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
        "    \n",
        "    # divide the number of joint elements by the number of all elements\n",
        "    overlap = len(joint_elems) / len(all_elems)\n",
        "    \n",
        "    return overlap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaFdv7YWvwGQ"
      },
      "source": [
        "# let's try using the function. remember the result here and compare with the next function below.\n",
        "jaccard_similarity([1, 2, 3], [1, 2, 3, 4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seRV_tAWvwGS"
      },
      "source": [
        "One of the more commonly used metrics in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and as shown in class, you can output the scores for both precision and recall when comparing two samples. To get the final score, you will want to compute the F1-score as given by:\n",
        "\n",
        "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$\n",
        "\n",
        "<a name=\"ex08\"></a>\n",
        "### Exercise 08\n",
        "\n",
        "**Instructions**: Implement the `rouge1_similarity()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "475iCOj7vwGT"
      },
      "source": [
        "# UNQ_C8\n",
        "# GRADED FUNCTION\n",
        "\n",
        "# for making a frequency table easily\n",
        "from collections import Counter\n",
        "\n",
        "def rouge1_similarity(system, reference):\n",
        "    \"\"\"Returns the ROUGE-1 score between two token lists\n",
        "\n",
        "    Args:\n",
        "        system (list of int): tokenized version of the system translation\n",
        "        reference (list of int): tokenized version of the reference translation\n",
        "\n",
        "    Returns:\n",
        "        float: overlap between the two token lists\n",
        "    \"\"\"    \n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
        "    \n",
        "    # make a frequency table of the system tokens (hint: use the Counter class)\n",
        "    sys_counter = Counter(system)\n",
        "    \n",
        "    # make a frequency table of the reference tokens (hint: use the Counter class)\n",
        "    ref_counter = Counter(reference)\n",
        "    \n",
        "    # initialize overlap to 0\n",
        "    overlap = 0\n",
        "    \n",
        "    # run a for loop over the sys_counter object (can be treated as a dictionary)\n",
        "    for token in sys_counter:\n",
        "        \n",
        "        # lookup the value of the token in the sys_counter dictionary (hint: use the get() method)\n",
        "        token_count_sys = sys_counter.get(token,0)\n",
        "        \n",
        "        # lookup the value of the token in the ref_counter dictionary (hint: use the get() method)\n",
        "        token_count_ref = ref_counter.get(token,0)\n",
        "        \n",
        "        # update the overlap by getting the smaller number between the two token counts above\n",
        "        overlap += min(token_count_sys, token_count_ref)\n",
        "    \n",
        "    # get the precision (i.e. number of overlapping tokens / number of system tokens)\n",
        "    precision = overlap / sum(sys_counter.values())\n",
        "    \n",
        "    # get the recall (i.e. number of overlapping tokens / number of reference tokens)\n",
        "    recall = overlap / sum(ref_counter.values())\n",
        "    \n",
        "    if precision + recall != 0:\n",
        "        # compute the f1-score\n",
        "        rouge1_score = 2 * ((precision * recall)/(precision + recall))\n",
        "    else:\n",
        "        rouge1_score = 0 \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return rouge1_score\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5-_-I6cvwGT"
      },
      "source": [
        "# notice that this produces a different value from the jaccard similarity earlier\n",
        "rouge1_similarity([1, 2, 3], [1, 2, 3, 4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-93HQ0mvwGT"
      },
      "source": [
        "# BEGIN UNIT TEST\n",
        "w1_unittest.test_rouge1_similarity(rouge1_similarity)\n",
        "# END UNIT TEST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UTYw7Q1vwGT"
      },
      "source": [
        "### 4.2.3 Overall score\n",
        "\n",
        "We will now build a function to generate the overall score for a particular sample. As mentioned earlier, we need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 to 30. Then, we compare sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.\n",
        "\n",
        "1. Get similarity score between sample 1 and sample 2\n",
        "2. Get similarity score between sample 1 and sample 3\n",
        "3. Get similarity score between sample 1 and sample 4\n",
        "4. Get average score of the first 3 steps. This will be the overall score of sample 1.\n",
        "5. Iterate and repeat until samples 1 to 4 have overall scores.\n",
        "\n",
        "We will be storing the results in a dictionary for easy lookups.\n",
        "\n",
        "<a name=\"ex09\"></a>\n",
        "### Exercise 09\n",
        "\n",
        "**Instructions**: Implement the `average_overlap()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvk278wEvwGW"
      },
      "source": [
        "# UNQ_C9\n",
        "# GRADED FUNCTION\n",
        "def average_overlap(similarity_fn, samples, *ignore_params):\n",
        "    \"\"\"Returns the arithmetic mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        similarity_fn (function): similarity function used to compute the overlap\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        *ignore_params: additional parameters will be ignored\n",
        "\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"  \n",
        "    \n",
        "    # initialize dictionary\n",
        "    scores = {}\n",
        "    \n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "        \n",
        "        ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
        "        \n",
        "        # initialize overlap to 0.0\n",
        "        overlap = 0.0\n",
        "        \n",
        "        # run a for loop for each sample\n",
        "        for index_sample, sample in enumerate(samples): \n",
        "\n",
        "            # skip if the candidate index is the same as the sample index\n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "                \n",
        "            # get the overlap between candidate and sample using the similarity function\n",
        "            sample_overlap = similarity_fn(candidate,sample)\n",
        "            \n",
        "            # add the sample overlap to the total overlap\n",
        "            overlap += sample_overlap\n",
        "            \n",
        "        # get the score for the candidate by computing the average\n",
        "        score = overlap/index_sample\n",
        "        \n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "        \n",
        "        ### END CODE HERE ###\n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnfW6VjevwGX"
      },
      "source": [
        "average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrQ56vluvwGX"
      },
      "source": [
        "# BEGIN UNIT TEST\n",
        "w1_unittest.test_average_overlap(average_overlap)\n",
        "# END UNIT TEST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyIgixqwvwGX"
      },
      "source": [
        "In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. We have implemented it below and you can use it in your experiements to see which one will give better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzDUcJeSvwGX"
      },
      "source": [
        "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
        "    \"\"\"Returns the weighted mean of each candidate sentence in the samples\n",
        "\n",
        "    Args:\n",
        "        samples (list of lists): tokenized version of the translated sentences\n",
        "        log_probs (list of float): log probability of the translated sentences\n",
        "\n",
        "    Returns:\n",
        "        dict: scores of each sample\n",
        "            key: index of the sample\n",
        "            value: score of the sample\n",
        "    \"\"\"\n",
        "    \n",
        "    # initialize dictionary\n",
        "    scores = {}\n",
        "    \n",
        "    # run a for loop for each sample\n",
        "    for index_candidate, candidate in enumerate(samples):    \n",
        "        \n",
        "        # initialize overlap and weighted sum\n",
        "        overlap, weight_sum = 0.0, 0.0\n",
        "        \n",
        "        # run a for loop for each sample\n",
        "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
        "\n",
        "            # skip if the candidate index is the same as the sample index            \n",
        "            if index_candidate == index_sample:\n",
        "                continue\n",
        "                \n",
        "            # convert log probability to linear scale\n",
        "            sample_p = float(np.exp(logp))\n",
        "\n",
        "            # update the weighted sum\n",
        "            weight_sum += sample_p\n",
        "\n",
        "            # get the unigram overlap between candidate and sample\n",
        "            sample_overlap = similarity_fn(candidate, sample)\n",
        "            \n",
        "            # update the overlap\n",
        "            overlap += sample_p * sample_overlap\n",
        "            \n",
        "        # get the score for the candidate\n",
        "        score = overlap / weight_sum\n",
        "        \n",
        "        # save the score in the dictionary. use index as the key.\n",
        "        scores[index_candidate] = score\n",
        "    \n",
        "    return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oH1Y4TjvwGY"
      },
      "source": [
        "weighted_avg_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOlPqaFjvwGY"
      },
      "source": [
        "### 4.2.4 Putting it all together\n",
        "\n",
        "We will now put everything together and develop the `mbr_decode()` function. Please use the helper functions you just developed to complete this. You will want to generate samples, get the score for each sample, get the highest score among all samples, then detokenize this sample to get the translated sentence.\n",
        "\n",
        "<a name=\"ex10\"></a>\n",
        "### Exercise 10\n",
        "\n",
        "**Instructions**: Implement the `mbr_overlap()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hPPERj0vwGY"
      },
      "source": [
        "# UNQ_C10\n",
        "# GRADED FUNCTION\n",
        "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
        "    \"\"\"Returns the translated sentence using Minimum Bayes Risk decoding\n",
        "\n",
        "    Args:\n",
        "        sentence (str): sentence to translate.\n",
        "        n_samples (int): number of samples to generate\n",
        "        score_fn (function): function that generates the score for each sample\n",
        "        similarity_fn (function): function used to compute the overlap between a pair of samples\n",
        "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "        vocab_file (str): filename of the vocabulary\n",
        "        vocab_dir (str): path to the vocabulary file\n",
        "\n",
        "    Returns:\n",
        "        str: the translated sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF `None` WITH YOUR CODE) ###\n",
        "    # generate samples\n",
        "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn, temperature, vocab_file, vocab_dir)\n",
        "    \n",
        "    # use the scoring function to get a dictionary of scores\n",
        "    # pass in the relevant parameters as shown in the function definition of \n",
        "    # the mean methods you developed earlier\n",
        "    scores = weighted_avg_overlap(jaccard_similarity, samples, log_probs)\n",
        "    \n",
        "    # find the key with the highest score\n",
        "    max_index = max(scores, key=scores.get)\n",
        "    \n",
        "    # detokenize the token list associated with the max_index\n",
        "    translated_sentence = detokenize(samples[max_index], vocab_file, vocab_dir)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    return (translated_sentence, max_index, scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ys3wh-zvwGY"
      },
      "source": [
        "TEMPERATURE = 1.0\n",
        "\n",
        "# put a custom string here\n",
        "your_sentence = 'She speaks English and German.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVjsx2fNvwGY"
      },
      "source": [
        "mbr_decode(your_sentence, 4, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7EJHaknvwGZ"
      },
      "source": [
        "mbr_decode('Congratulations!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMUVAyMbvwGZ"
      },
      "source": [
        "mbr_decode('You have completed the assignment!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZpenYK6vwGZ"
      },
      "source": [
        "**This unit test take a while to run. Please be patient**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2NXIIFPvwGZ"
      },
      "source": [
        "# BEGIN UNIT TEST\n",
        "w1_unittest.test_mbr_decode(mbr_decode, model)\n",
        "# END UNIT TEST"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abnBWUc-Nhxn"
      },
      "source": [
        "## For Next Updates\n",
        "**Note to myself on subwords**: This is definetely not an efficient way to generate subwords. I should rewrite this part, I can actually use Tensor2Tensor and with [Tensor](https://www.tensorflow.org/text/guide/subwords_tokenizer)\n",
        "\n",
        "the ende_32k subword vocab was generated using Tensor2Tensor - when you generate the WMT en-de data, it creates the vocab. I think it's quite standard these days, e.g., it's used for MLPerf. \n",
        "\n",
        "Here is a link for ende_32k.subword file: https://github.com/mlperf/training/tree/master/translation/tensorflow/transformer/vocab, you can use it directly."
      ]
    }
  ]
}